{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of PySpark_H_W_Session_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK1nI-VcQEUw"
      },
      "source": [
        "# PySpark H.W Session 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBrsJ9WgQEU0"
      },
      "source": [
        "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the future machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\", but more on that later!\n",
        "\n",
        "For now, just answer the questions and complete the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klOLe4_bQEU3"
      },
      "source": [
        "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C9viFsYmU1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe07f144-3aa0-498f-ac2a-6e4986b9e0e3"
      },
      "source": [
        "%config Completer.use_jedi = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr3r3sJrlXqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d81f5d-e6a5-4b1d-fb78-732b123131de"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 64 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=b4db5f3d2b72dbcd111bd06085afe09fc0e42847bd5865619941aa38fcb0b118\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqQDG08GlMXT"
      },
      "source": [
        "import pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_TAsagOlGYZ"
      },
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBTm6mJjQEU4"
      },
      "source": [
        "#### Start a simple Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AFIdTSZaQEU6"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43RyB3TsmP5g"
      },
      "source": [
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SvK1u-xQEU7"
      },
      "source": [
        "#### Load the Walmart Stock CSV File, have Spark infer the data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PFufNumHQEU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25df80b4-5752-4b3c-92a1-c9fb45f700e8"
      },
      "source": [
        "df = spark.read.csv('walmart_stock.csv',header=True,inferSchema=True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Date: string, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyuYG3HQEU9"
      },
      "source": [
        "#### What are the column names?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_79f3hne1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d145c8-b998-49fb-86b3-6ae734446a9c"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF87H3r1QEU_"
      },
      "source": [
        "#### What does the Schema look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaM8KQGTQEU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01cb90c-b68d-46bb-dc66-cd482dafd44b"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixYkVStcQEVA"
      },
      "source": [
        "#### Print out the first 5 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYPAuAWenzSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9056ac19-3b17-450b-d950-b31a9fca5004"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
              " Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
              " Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
              " Row(Date='2012-01-06', Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
              " Row(Date='2012-01-09', Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5biK3B4QEVB"
      },
      "source": [
        "#### Use describe() to learn about the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28fLXv5QoGCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d75e470-a3ed-4eba-cc44-c1a52ced09dc"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
            "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1nK57i1QEVC"
      },
      "source": [
        "## Bonus Question!\n",
        "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
        "\n",
        "If you get stuck on this, don't worry, just view the solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciAYhyY7QEVC",
        "outputId": "bdec4412-7898-46ac-df9a-5205859ad82a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- summary: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vuKIj4WPQEVD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9QtiMvsQEVD",
        "outputId": "5e18ca18-6888-41e0-8e69-8e3aea746ee5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+--------+--------+--------+--------+\n",
            "|summary|    Open|    High|     Low|   Close|  Volume|\n",
            "+-------+--------+--------+--------+--------+--------+\n",
            "|  count|1,258.00|1,258.00|1,258.00|1,258.00|    1258|\n",
            "|   mean|   72.36|   72.84|   71.92|   72.39| 8222093|\n",
            "| stddev|    6.77|    6.77|    6.74|    6.76| 4519781|\n",
            "|    min|   56.39|   57.06|   56.30|   56.42| 2094900|\n",
            "|    max|   90.80|   90.97|   89.25|   90.47|80898100|\n",
            "+-------+--------+--------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MwLgnCSQEVD"
      },
      "source": [
        "#### Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbcB_9Jxtn3F"
      },
      "source": [
        "newdf = df.withColumn(\"HV Ratio\",df['High']/df['Volume'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2yQbBOapIW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd4a00c-9eb3-44f0-f292-5b94d2b9247b"
      },
      "source": [
        "newdf.select(['HV Ratio']).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            HV Ratio|\n",
            "+--------------------+\n",
            "|4.819714653321546E-6|\n",
            "|6.290848613094555E-6|\n",
            "|4.669412994783916E-6|\n",
            "|7.367338463826307E-6|\n",
            "|8.915604778943901E-6|\n",
            "|8.644477436914568E-6|\n",
            "|9.351828421515645E-6|\n",
            "| 8.29141562102703E-6|\n",
            "|7.712212102001476E-6|\n",
            "|7.071764823529412E-6|\n",
            "|1.015495466386981E-5|\n",
            "|6.576354146362592...|\n",
            "| 5.90145296180676E-6|\n",
            "|8.547679455011844E-6|\n",
            "|8.420709512685392E-6|\n",
            "|1.041448341728929...|\n",
            "|8.316075414862431E-6|\n",
            "|9.721183814992126E-6|\n",
            "|8.029436027707578E-6|\n",
            "|6.307432259386365E-6|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfOdDUdCQEVE"
      },
      "source": [
        "#### What day had the Peak High in Price?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP6itJofvmJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "53244da8-f98a-453a-c8a8-3e08b2e5a6da"
      },
      "source": [
        "newdf.orderBy(newdf['High'].desc()).head(1)[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2015-01-13'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_RIIXHnQEVE"
      },
      "source": [
        "#### What is the mean of the Close column?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsbiuuLvwlc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58a779f-a131-4283-d744-a467f93997fe"
      },
      "source": [
        "newdf.groupby('Close').avg().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+-----------------+-----------------+----------+-----------------+------------------+--------------------+\n",
            "|    Close|        avg(Open)|        avg(High)|         avg(Low)|avg(Close)|      avg(Volume)|    avg(Adj Close)|       avg(HV Ratio)|\n",
            "+---------+-----------------+-----------------+-----------------+----------+-----------------+------------------+--------------------+\n",
            "|     74.5|        74.839996|        74.989998|        74.260002|      74.5|        8535700.0|         66.213137|8.785453799922678E-6|\n",
            "|71.309998|71.24666866666666|        71.693334|70.84333566666668| 71.309998|8214433.333333333|         65.798023|9.212998576608988E-6|\n",
            "|    78.75|            78.25|            78.75|             78.0|     78.75|        6123900.0|         71.257698|1.285945230980257...|\n",
            "|70.870003|            70.57|        71.050003|        70.519997| 70.870003|        6363600.0| 69.37782299999999|1.116506427179584E-5|\n",
            "|75.339996|74.95500200000001|            75.43|       74.8449975| 75.339996|        8903850.0|         69.860258|1.251019906891522...|\n",
            "|70.980003|            71.07|        71.639999|        70.599998| 70.980003|        1.78448E7|         70.461903|4.014614845781404E-6|\n",
            "|61.900002|            61.68|            61.91|        61.380001| 61.900002|        5501000.0|53.988572999999995|1.125431739683693...|\n",
            "|    71.43|        72.300003|        72.339996|            71.43|     71.43|        7634200.0|         63.484623|9.475779518482617E-6|\n",
            "|74.220001|        74.309998|        74.440002|        74.010002| 74.220001|        3928200.0|          69.27512|1.895015579654804E-5|\n",
            "|62.029999|            62.32|            62.43|        61.700001| 62.029999|        7727200.0| 54.10195600000001|8.079252510611865E-6|\n",
            "|72.309998|       72.0999985|       72.6100005|       71.7799985| 72.309998|        8543600.0|         64.093887|8.828705102784408E-6|\n",
            "|86.639999|       86.2799985|        86.990002|       86.0550005| 86.639999|        5889700.0|         81.319416|1.739065031309511E-5|\n",
            "|     67.0|        66.860001|       67.5149995|       66.3149985|      67.0|       1.005155E7|        64.8733275|8.092350202324013E-6|\n",
            "|73.839996|73.72999949999999|        74.014999|       73.3849985| 73.839996|        7292450.0| 72.28528299999999|1.084182371528176...|\n",
            "|73.730003|        73.709999|            74.07|        73.410004| 73.730003|        5248300.0| 72.17760600000001|1.411314139816702...|\n",
            "|77.970001|        78.244999|78.33000150000001|       77.3899995| 77.970001|        6016350.0| 70.99268699999999|1.329470663942718...|\n",
            "|77.010002|           77.035|       77.5699995|       76.6749995| 77.010002|        6395700.0|         71.408798|1.234955535567552...|\n",
            "|76.489998|            76.25|            76.57|        75.860001| 76.489998|        3752900.0|         71.393879|2.040288843294518...|\n",
            "|87.720001|            87.07|        87.720001|        86.269997| 87.720001|        6522800.0| 82.33309399999999|1.344821257742074E-5|\n",
            "|     69.0|69.16000133333334|        69.433332|        68.516665|      69.0|        9016200.0| 61.55178766666666|7.761773219605772E-6|\n",
            "+---------+-----------------+-----------------+-----------------+----------+-----------------+------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W9kgKR7QEVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "905046a3-e706-4b90-cffb-a20b92df5dd4"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "newdf.select(mean('Close')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|       avg(Close)|\n",
            "+-----------------+\n",
            "|72.38844998012726|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8V-r7I1QEVF"
      },
      "source": [
        "#### What is the max and min of the Volume column?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "z5mrVE6WQEVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25135404-810c-4dce-e50a-43da34023109"
      },
      "source": [
        "from pyspark.sql.functions import max, min\n",
        "newdf.select(max('Volume'), min('Volume')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|max(Volume)|min(Volume)|\n",
            "+-----------+-----------+\n",
            "|   80898100|    2094900|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT4RVThzQEVG"
      },
      "source": [
        "#### How many days was the Close lower than 60 dollars?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukyn4th3QEVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59b16b5-1906-4db4-e9cb-8c7923598b4f"
      },
      "source": [
        "newdf.filter(newdf['Close'] < 60).count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pqovtOeQEVG"
      },
      "source": [
        "#### What percentage of the time was the High greater than 80 dollars ?\n",
        "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQqYozIUycqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d641af3-f174-43a5-ddde-07df9ad5cd62"
      },
      "source": [
        "((newdf.filter(newdf['High'] > 80).count()) / (newdf.count())) * 100.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.141494435612083"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhggc4nrQEVH"
      },
      "source": [
        "#### What is the Pearson correlation between High and Volume?\n",
        "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSmeC1UJyugl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4411459-6efa-49b0-ad00-0abe11f0774a"
      },
      "source": [
        "from pyspark.sql.functions import corr\n",
        "newdf.select(corr('High','Volume')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "| corr(High, Volume)|\n",
            "+-------------------+\n",
            "|-0.3384326061737161|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cUkJpW8QEVJ"
      },
      "source": [
        "#### What is the max High per year?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX3A9WxbQEVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf98677b-1daf-4496-ec70-ee22e9495373"
      },
      "source": [
        "from pyspark.sql.functions import year\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "newdf = newdf.withColumn(\"Year\",year(newdf['Date']))\n",
        "newdf.groupBy('Year').agg(F.max('High')).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|Year|max(High)|\n",
            "+----+---------+\n",
            "|2015|90.970001|\n",
            "|2013|81.370003|\n",
            "|2014|88.089996|\n",
            "|2012|77.599998|\n",
            "|2016|75.190002|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzrj0hnQQEVK"
      },
      "source": [
        "#### What is the average Close for each Calendar Month?\n",
        "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK8b-dI47BXK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fff3b4-9668-4bb3-f9f4-99de61388f3e"
      },
      "source": [
        "from pyspark.sql.functions import month\n",
        "\n",
        "newdf = newdf.withColumn(\"Month\",month(newdf['Date']))\n",
        "newdf.groupBy('Month').agg(F.avg('Close')).show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|Month|       avg(Close)|\n",
            "+-----+-----------------+\n",
            "|   12|72.84792478301885|\n",
            "|    1|71.44801958415842|\n",
            "|    6| 72.4953774245283|\n",
            "|    3|71.77794377570092|\n",
            "|    5|72.30971688679247|\n",
            "|    9|72.18411785294116|\n",
            "|    4|72.97361900952382|\n",
            "|    8|73.02981855454546|\n",
            "|    7|74.43971943925233|\n",
            "|   10|71.57854545454543|\n",
            "|   11| 72.1110893069307|\n",
            "|    2|  71.306804443299|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}